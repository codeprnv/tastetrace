{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gByLAcmtQwT"
      },
      "source": [
        "#NOTE: To make it easier for us to manage datasets, images and models we create a HOME constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cvWx84CRtUiF"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCd3K95gtasr"
      },
      "source": [
        "#Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UjVVOhmTtjin"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install tqdm\n",
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "%cd {HOME}\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_FsMCJDtt9y"
      },
      "source": [
        "#Install the dependencies of grounding dino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "A1QnCWh6tyKI"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "%cd {HOME}/GroundingDINO\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJAkSEPruqTF"
      },
      "source": [
        "#Download Grounding DINO Model Weights\n",
        "\n",
        "To run Grounding DINO we need two files - configuration and model weights. The configuration file is part of the Grounding DINO repository, which we have already cloned. The weights file, on the other hand, we need to download. We write the paths to both files to the GROUNDING_DINO_CONFIG_PATH and GROUNDING_DINO_CHECKPOINT_PATH variables and verify if the paths are correct and the files exist on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c6OgX5bouwX9"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "\n",
        "GROUNDING_DINO_CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(GROUNDING_DINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CONFIG_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R6nut7W_u3EZ"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V1ZTgLHhu5Sv"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"groundingdino_swint_ogc.pth\")\n",
        "print(GROUNDING_DINO_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnFtUSH4u88V"
      },
      "source": [
        "#Download Segment Anything Model (SAM) Weights\n",
        "As with Grounding DINO, in order to run SAM we need a weights file, which we must first download. We write the path to local weight file to SAM_CHECKPOINT_PATH variable and verify if the path is correct and the file exist on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FZyoZCSIu_0D"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jQUZuuQIvGZh"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "\n",
        "SAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(SAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(SAM_CHECKPOINT_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKVQDwpdvKpE"
      },
      "source": [
        "#Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_puyXKxXvMr4"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Change to the correct directory\n",
        "%cd /content/GroundingDINO\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from groundingdino.util.inference import Model\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# Ensure no CUDA is being used\n",
        "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "#os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
        "\n",
        "# Set the device to CPU\n",
        "#DEVICE = torch.device('cpu')\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH, device=DEVICE)\n",
        "\n",
        "# Load the SAM model\n",
        "SAM_ENCODER_VERSION = \"vit_h\"\n",
        "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
        "sam_predictor = SamPredictor(sam)\n",
        "\n",
        "print(\"Models loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GBwfJxPQPfaI"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install --upgrade sympy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mYhovdTuH_e"
      },
      "source": [
        "#Ensure GPU access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "di9fccWluIj6"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"Checking GPU availability...\")\n",
        "gpu_available = torch.cuda.is_available()\n",
        "print(\"GPU available:\", gpu_available)\n",
        "if not gpu_available:\n",
        "    print(\"Please enable GPU support for better performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7T8DBauuQXy"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSDbLLXzuWdn"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def enhance_class_name(class_names):\n",
        "    return [f\"all {class_name}s\" for class_name in class_names]\n",
        "\n",
        "def segment(sam_predictor, image, xyxy):\n",
        "    sam_predictor.set_image(image)\n",
        "    result_masks = []\n",
        "    for box in xyxy:\n",
        "        masks, scores, logits = sam_predictor.predict(box=box, multimask_output=True)\n",
        "        index = np.argmax(scores)\n",
        "        result_masks.append(masks[index])\n",
        "    return np.array(result_masks)\n",
        "\n",
        "def list_files_with_extensions(directory, extensions):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(tuple(extensions)):\n",
        "                file_paths.append(os.path.join(root, file))\n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkeUjNJtwF3U"
      },
      "source": [
        "#Full Dataset Mask Auto Annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izw-YSf0wQqx"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def full_dataset_annotation(images_directory, classes, box_threshold, text_threshold):\n",
        "    images = {}\n",
        "    annotations = {}\n",
        "    image_paths = list_files_with_extensions(directory=images_directory, extensions=['jpg', 'jpeg', 'png'])\n",
        "\n",
        "    print(f\"Found {len(image_paths)} image(s) in {images_directory}\")\n",
        "\n",
        "    for image_path in tqdm(image_paths):\n",
        "        print(f\"Processing image: {image_path}\")\n",
        "        image_name = os.path.basename(image_path)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        if image is None:\n",
        "            print(f\"Failed to read image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        detections = grounding_dino_model.predict_with_classes(\n",
        "            image=image,\n",
        "            classes=enhance_class_name(class_names=classes),\n",
        "            box_threshold=box_threshold,\n",
        "            text_threshold=text_threshold\n",
        "        )\n",
        "        if detections is None or len(detections.xyxy) == 0:\n",
        "            print(f\"No detections for image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        detections.mask = segment(sam_predictor=sam_predictor, image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB), xyxy=detections.xyxy)\n",
        "        images[image_name] = image\n",
        "        annotations[image_name] = detections\n",
        "    print(f\"Processed {len(images)} image(s)\")\n",
        "    return images, annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZo1ETP734f_"
      },
      "source": [
        "#Prompt the user for class names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D2k8Sd8W35tS"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def get_class_names():\n",
        "    while True:\n",
        "        class_names_input = input(\"Please enter the class names, separated by commas (e.g., dog, cat, car): \")\n",
        "        class_names = [name.strip() for name in class_names_input.split(\",\") if name.strip()]\n",
        "        if class_names:\n",
        "            return class_names\n",
        "        else:\n",
        "            print(\"You must enter at least one class name.\")\n",
        "\n",
        "class_names = get_class_names()\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTc325DlwU-G"
      },
      "source": [
        "\n",
        "# Ensure the images directory exists in Colab\n",
        "# Upload or ensure images are present in '/content/folder_name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IoyrZu2cwbjn"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "print(os.getcwd())\n",
        "images_directory = os.path.join(HOME, '/content/annotation_images/Cauliflower')\n",
        "os.makedirs(images_directory, exist_ok=True)\n",
        "print(class_names)\n",
        "images, annotations = full_dataset_annotation(images_directory, class_names, 0.35, 0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diXkP3NvxEvJ"
      },
      "source": [
        "# Check if images and annotations are generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KJI8-En8xHuk"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(f\"Number of images: {len(images)}\")\n",
        "print(f\"Number of annotations: {len(annotations)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR0RbUq54Ci9"
      },
      "source": [
        "# Display 2-3 random annotated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c87oVMNs23iq"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import supervision as sv\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_random_annotated_images(images, annotations, class_names, num_images=3):\n",
        "    plot_images = []\n",
        "    plot_titles = []\n",
        "\n",
        "    box_annotator = sv.BoxAnnotator()\n",
        "    mask_annotator = sv.MaskAnnotator()\n",
        "\n",
        "    selected_images = random.sample(list(annotations.keys()), min(num_images, len(annotations)))\n",
        "\n",
        "    for image_name in selected_images:\n",
        "        image = images[image_name]\n",
        "        detections = annotations[image_name]\n",
        "\n",
        "        # Annotate the image with masks\n",
        "        annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "\n",
        "        # Annotate the image with bounding boxes\n",
        "        annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "\n",
        "        # Add labels to the annotated image using OpenCV\n",
        "        for i in range(len(detections.class_id)):\n",
        "            class_id = detections.class_id[i]\n",
        "            confidence = detections.confidence[i]\n",
        "            bbox = detections.xyxy[i]  # Assuming detections has an attribute xyxy\n",
        "\n",
        "            label = f\"{class_names[class_id]}: {confidence:.2f}\"\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "\n",
        "            # Draw the label above the bounding box\n",
        "            cv2.putText(annotated_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        plot_images.append(annotated_image)\n",
        "        title = \" \".join(set([\n",
        "            str(class_names[class_id])\n",
        "            for class_id\n",
        "            in detections.class_id\n",
        "        ]))\n",
        "        plot_titles.append(title)\n",
        "\n",
        "    # Display the images in a grid\n",
        "    fig, axs = plt.subplots(1, min(num_images, len(annotations)), figsize=(15, 5))\n",
        "    for idx, img in enumerate(plot_images):\n",
        "        axs[idx].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        axs[idx].set_title(plot_titles[idx])\n",
        "        axs[idx].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming images, annotations, and class_names are already defined\n",
        "display_random_annotated_images(images, annotations, class_names, num_images=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsElj_vL4sxe"
      },
      "source": [
        "#Prompt user to enter main folder name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0dr6AUIFxRp8"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "main_folder_name = input(\"Please enter the main folder name for the dataset: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ7gFRwZxV8u"
      },
      "source": [
        "# Create directories in YOLO format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq7llW47xaxM"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "def create_yolo_directories(main_folder_name):\n",
        "    base_path = os.path.join(HOME, main_folder_name)\n",
        "    for split in ['train', 'test', 'valid']:\n",
        "        os.makedirs(os.path.join(base_path, split, 'images'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(base_path, split, 'labels'), exist_ok=True)\n",
        "    return base_path\n",
        "\n",
        "base_path = create_yolo_directories(main_folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbaAdcPhxhot"
      },
      "source": [
        "# Save labels in YOLO format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n58ic8m_xix7"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def save_annotations(images, annotations, classes, base_path, split):\n",
        "    images_path = os.path.join(base_path, split, 'images')\n",
        "    labels_path = os.path.join(base_path, split, 'labels')\n",
        "\n",
        "    for image_name, detections in annotations.items():\n",
        "        # Save image\n",
        "        image = images[image_name]\n",
        "        image_output_path = os.path.join(images_path, image_name)\n",
        "        cv2.imwrite(image_output_path, image)\n",
        "        print(f\"Saved image to {image_output_path}\")\n",
        "\n",
        "        # Save annotations\n",
        "        annotation_output_path = os.path.join(labels_path, f\"{os.path.splitext(image_name)[0]}.txt\")\n",
        "        with open(annotation_output_path, 'w') as f:\n",
        "            for bbox, confidence, class_id, mask in zip(detections.xyxy, detections.confidence, detections.class_id, detections.mask):\n",
        "                x_center = (bbox[0] + bbox[2]) / 2 / image.shape[1]\n",
        "                y_center = (bbox[1] + bbox[3]) / 2 / image.shape[0]\n",
        "                width = (bbox[2] - bbox[0]) / image.shape[1]\n",
        "                height = (bbox[3] - bbox[1]) / image.shape[0]\n",
        "                f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "        print(f\"Saved annotations to {annotation_output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FubrhYZxrr_"
      },
      "source": [
        "# Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7zduMyG9xsq3"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "train_ratio = 0.8\n",
        "valid_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "image_paths = list(images.keys())\n",
        "np.random.shuffle(image_paths)\n",
        "train_split = int(len(image_paths) * train_ratio)\n",
        "valid_split = int(len(image_paths) * (train_ratio + valid_ratio))\n",
        "\n",
        "train_images = {k: images[k] for k in image_paths[:train_split]}\n",
        "valid_images = {k: images[k] for k in image_paths[train_split:valid_split]}\n",
        "test_images = {k: images[k] for k in image_paths[valid_split:]}\n",
        "\n",
        "train_annotations = {k: annotations[k] for k in image_paths[:train_split]}\n",
        "valid_annotations = {k: annotations[k] for k in image_paths[train_split:valid_split]}\n",
        "test_annotations = {k: annotations[k] for k in image_paths[valid_split:]}\n",
        "\n",
        "print(\"Saving training annotations...\")\n",
        "save_annotations(train_images, train_annotations, class_names, base_path, 'train')\n",
        "print(\"Saving validation annotations...\")\n",
        "save_annotations(valid_images, valid_annotations, class_names, base_path, 'valid')\n",
        "print(\"Saving test annotations...\")\n",
        "save_annotations(test_images, test_annotations, class_names, base_path, 'test')\n",
        "\n",
        "print(f\"YOLO dataset has been created in {base_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ZUpeBP45Jn"
      },
      "source": [
        "#Download files from collab in zip format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mGiDYaqU48i2"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import locale\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def set_utf8_locale():\n",
        "    \"\"\"Sets the locale to UTF-8.\"\"\"\n",
        "    try:\n",
        "        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')  # Try en_US.UTF-8 first\n",
        "    except locale.Error:\n",
        "        try:\n",
        "            locale.setlocale(locale.LC_ALL, 'UTF-8')  # Try UTF-8 as a fallback\n",
        "        except locale.Error:\n",
        "            print(\"Warning: Could not set locale to UTF-8. File names might be corrupted.\")\n",
        "\n",
        "# Call the function before running zip command\n",
        "set_utf8_locale()\n",
        "\n",
        "# Define the directory to be zipped and the output zip file path\n",
        "input_directory = '/content/Annotated_Cauliflower'\n",
        "output_zip_file = '/content/Cauliflower.zip'\n",
        "\n",
        "# Create a ZIP file using shutil.make_archive\n",
        "shutil.make_archive(output_zip_file.replace('.zip', ''), 'zip', input_directory)\n",
        "\n",
        "# Download the created ZIP file\n",
        "files.download(output_zip_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmHKBsi9mfEn"
      },
      "source": [
        "# Install the yolo packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ICnCO38Ymjyd"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import locale\n",
        "\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# Install necessary packages without locale checks\n",
        "!pip install --no-deps ultralytics opencv-python-headless\n",
        "\n",
        "# Verify installation by importing the packages\n",
        "import ultralytics\n",
        "import cv2\n",
        "\n",
        "print(\"Packages installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnXb9aKLmn8B"
      },
      "source": [
        "# Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APBf3NckmrmV"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeRibupSmut5"
      },
      "source": [
        "# Load the YOLO model (pre-trained or custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw3p5_E0mwSj"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def load_model(model_path=None):\n",
        "    \"\"\"\n",
        "    Load YOLO model (either from a pretrained or custom path)\n",
        "    \"\"\"\n",
        "    if model_path:\n",
        "        model = YOLO(model_path)\n",
        "    else:\n",
        "        model = YOLO('yolov10s.pt')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KI4Ljdgm0k9"
      },
      "source": [
        "# Function to train the YOLO model with custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWs2k7qVm1Rc"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def train_model(yaml_path, model, epochs=25, batch=16, imgsz=640):\n",
        "    \"\"\"\n",
        "    Train the YOLO model with the given parameters.\n",
        "    \"\"\"\n",
        "    # Start training\n",
        "    results = model.train(\n",
        "        data=yaml_path,\n",
        "        epochs=epochs,\n",
        "        batch=batch,\n",
        "        imgsz=imgsz,\n",
        "    )\n",
        "\n",
        "    # Get the path to the trained weights\n",
        "    last_weights_path = results.save_dir / \"weights\" / \"last.pt\"  # Path to the last weights\n",
        "    best_weights_path = results.save_dir / \"weights\" / \"best.pt\"  # Path to the best weights\n",
        "\n",
        "    print(f\"Training completed. Model is saved at:\")\n",
        "    print(f\"Last weights: {last_weights_path}\")\n",
        "    print(f\"Best weights: {best_weights_path}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jq2YWx_m9kY"
      },
      "source": [
        "# Function for inference on a single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZKaWnISnA3Z"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def detect_on_image(model, image_path):\n",
        "    \"\"\"\n",
        "    Perform detection on a single image with improved label placement.\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Error: Unable to load image at {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Perform detection\n",
        "    results = model(image)\n",
        "\n",
        "    # Draw bounding boxes and labels\n",
        "    for box in results[0].boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Box coordinates\n",
        "        label = model.names[int(box.cls[0])]  # Class name\n",
        "        confidence = box.conf[0]  # Confidence score\n",
        "\n",
        "        # Prepare the label text\n",
        "        label_text = f\"{label} ({confidence:.2f})\"\n",
        "\n",
        "        # Get text size\n",
        "        (text_width, text_height), baseline = cv2.getTextSize(\n",
        "            label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n",
        "        )\n",
        "\n",
        "        # Adjust label position to stay within bounds\n",
        "        label_y = max(y1 - text_height - baseline, 0)  # Stay within image bounds\n",
        "        label_x = x1\n",
        "\n",
        "        # Draw a filled rectangle behind the text\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (label_x, label_y),\n",
        "            (label_x + text_width, label_y + text_height + baseline),\n",
        "            (0, 255, 0),  # Background color (green)\n",
        "            -1,  # Filled rectangle\n",
        "        )\n",
        "\n",
        "        # Draw the bounding box\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        # Draw the label text\n",
        "        cv2.putText(\n",
        "            image,\n",
        "            label_text,\n",
        "            (label_x, label_y + text_height),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.5,\n",
        "            (0, 0, 0),  # Text color (black for contrast)\n",
        "            1,\n",
        "        )\n",
        "\n",
        "    # Convert BGR image to RGB\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the result using matplotlib\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0IHax8snDU_"
      },
      "source": [
        "# Function for real-time detection using the webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8fXbF_QnFZC"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def real_time_detection(model):\n",
        "    \"\"\"\n",
        "    Perform real-time ingredient detection using the webcam.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(0)  # Open default camera\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not access the camera.\")\n",
        "        return\n",
        "\n",
        "    print(\"Press 'q' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"Error: Could not read frame.\")\n",
        "            break\n",
        "\n",
        "        # Perform detection\n",
        "        results = model(frame)\n",
        "\n",
        "        # Draw bounding boxes and labels\n",
        "        for box in results[0].boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Box coordinates\n",
        "            label = model.names[int(box.cls[0])]  # Class name\n",
        "            confidence = box.conf[0]  # Confidence score\n",
        "\n",
        "            # Draw the bounding box and label\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                f\"{label} ({confidence:.2f})\",\n",
        "                (x1, y1 - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.5,\n",
        "                (0, 255, 0),\n",
        "                2,\n",
        "            )\n",
        "\n",
        "        # Display the frame\n",
        "        cv2.imshow(\"Ingredient Detection\", frame)\n",
        "\n",
        "        # Quit if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roM5vzFdnHjA"
      },
      "source": [
        "# Save the YOLO model for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpT4ED8rnJDT"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def save_model(model, output_path=\"/content/saved_model\"):\n",
        "    \"\"\"\n",
        "    Save the trained YOLO model for future use.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    model.save(os.path.join(output_path, \"trained_model.pt\"))\n",
        "    print(f\"Model saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rjLlyLofufOB"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW46EeIlnMy2"
      },
      "source": [
        "# Main function to train, detect and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vGuG-1POnO3n"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def main(mode=\"image\", yaml_path=None, image_path=None, save=False, output_path=\"/content/saved_model\"):\n",
        "    if not yaml_path:\n",
        "        print(\"Error: Provide a valid YAML file path for training.\")\n",
        "        return\n",
        "\n",
        "    # Load the model (either pre-trained or custom)\n",
        "    model = load_model('/content/saved_model/trained_model.pt')\n",
        "    # Uncomment the following line if you need to train the model\n",
        "    model = train_model(yaml_path, model)\n",
        "\n",
        "    # Perform detection based on the user's choice\n",
        "    if mode == \"image\" and image_path:\n",
        "        detect_on_image(model, image_path)\n",
        "    elif mode == \"realtime\":\n",
        "        real_time_detection(model)\n",
        "    else:\n",
        "        print(\"Error: Invalid mode. Choose 'image' or 'realtime'.\")\n",
        "\n",
        "    # Save the trained model if requested\n",
        "    if save:\n",
        "        save_model(model, output_path)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: Detect ingredients from an image or real-time\n",
        "    # Main mode can be 'image' or 'realtime'\n",
        "\n",
        "    # main(mode=\"image\", yaml_path=\"path_to_your_dataset.yaml\", image_path=\"path_to_image.jpg\", save=True)\n",
        "    # Or use real-time mode\n",
        "    main(mode=\"image\", yaml_path=\"/content/data.yaml\", image_path=\"/content/train/images/0010_Cabbage_train.jpg\", save=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fRUgdJFYzCgr"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the trained model\n",
        "model = YOLO('/content/saved_model/trained_model.pt')\n",
        "\n",
        "# Load an image\n",
        "image_path = '/content/train/images/0020_Brinjal_train.jpg'\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Run inference\n",
        "results = model(image)\n",
        "\n",
        "# Display results for each detection\n",
        "for result in results:\n",
        "    result.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6Tf2X-lL0jO3"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Specify the directory you want to delete\n",
        "dir_path = '/content/train'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(dir_path) and os.path.isdir(dir_path):\n",
        "    # Delete the directory\n",
        "    shutil.rmtree(dir_path)\n",
        "    print(f\"Directory '{dir_path}' has been deleted.\")\n",
        "else:\n",
        "    print(f\"Directory '{dir_path}' does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rFhRNv_Z5gr"
      },
      "source": [
        "# Convert model to tflite format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tHbGlq1AZ9Kx"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Install YOLOv8 and related tools\n",
        "!pip install ultralytics onnx onnx-simplifier tensorflow tf2onnx onnx-tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czLnwn9lax9U"
      },
      "source": [
        "#Upload Your YOLOv8 .pt Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rvPOjpVFa1oD"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload your `.pt` file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgPIOl5sa4F3"
      },
      "source": [
        "#Export the YOLOv8 Model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvWGJMUPa6AT"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load your YOLOv8 model\n",
        "model = YOLO('trained_model.pt')  # Replace with your uploaded .pt filename\n",
        "\n",
        "# Export the model to ONNX\n",
        "model.export(format='onnx', opset=12)  # Adjust opset if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0t7A5g9a9Ee"
      },
      "source": [
        "#Simplify the ONNX Model (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tH24Y4ubAHI"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!python -m onnxsim your_model.onnx simplified_model.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79XCHUWFbBha"
      },
      "source": [
        "#Convert ONNX to TensorFlow SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LJVk_qGLbD8M"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#!pip uninstall -y tensorflow keras\n",
        "#!pip install tensorflow==2.12 keras==2.12\n",
        "#!pip install onnx onnx-tf\n",
        "#!pip install tensorflow-probability==0.20.0\n",
        "\n",
        "import onnx\n",
        "import tensorflow as tf\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(\"trained_model.onnx\")\n",
        "\n",
        "# Convert ONNX model to TensorFlow SavedModel\n",
        "tf_rep = prepare(onnx_model)\n",
        "\n",
        "# Cast all the weights and biases in the TensorFlow model to float32\n",
        "for var in tf_rep.tf_module.variables:\n",
        "    var.assign(tf.cast(var, tf.float32))\n",
        "\n",
        "# Save the model in float32 precision\n",
        "tf_rep.export_graph(\"/content/saved_model_dir_float32\")\n",
        "\n",
        "# Verify by loading the model and checking the dtype\n",
        "saved_model = tf.saved_model.load(\"/content/saved_model_dir_float32\")\n",
        "print(saved_model.signatures)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZoEqfaMhNb8"
      },
      "source": [
        "#Verify Model Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTIc_tXPhP13"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.saved_model.load('/content/saved_model_dir/')\n",
        "\n",
        "# Inspect the model's signature to find the input tensor name\n",
        "print(loaded_model.signatures)  # This will show the signature, including input tensor names\n",
        "\n",
        "# Get the signature for serving\n",
        "infer = loaded_model.signatures[\"serving_default\"]\n",
        "\n",
        "# Check the input signature name\n",
        "print(infer.structured_input_signature)\n",
        "\n",
        "# Define the correct input signature based on the inspected signature\n",
        "@tf.function(input_signature=[tf.TensorSpec(shape=[None, 640, 640, 3], dtype=tf.float32)])\n",
        "def predict(input_tensor):\n",
        "    # Call the model using the correct input signature\n",
        "    return infer(images=input_tensor)  # 'images' is typically the name of the input tensor for image models\n",
        "\n",
        "# Add the signature when saving the model\n",
        "signatures = {'serving_default': predict.get_concrete_function()}\n",
        "\n",
        "# Save the model with the new signature\n",
        "tf.saved_model.save(loaded_model, '/content/saved_model_dir_float32', signatures=signatures)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFKE_LggbKnx"
      },
      "source": [
        "#Convert the TensorFlow SavedModel to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w09fB6XYbLhm"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the TensorFlow SavedModel\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/saved_model_dir_float32/')\n",
        "\n",
        "# Enable optimizations (optional)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Enable TensorFlow Select Ops\n",
        "converter.allow_custom_ops = True\n",
        "\n",
        "# Convert to TFLite format\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('tastetrace.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWoi8kgwbNto"
      },
      "source": [
        "# Download the TFLite Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o0geujY6bQ6W"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from google.colab import files\n",
        "files.download('tastetrace.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8DuQVGTy1-K"
      },
      "source": [
        "# Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vyC1zBlpy38E"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLO11 model\n",
        "model = YOLO(\"/content/trained_model.pt\")\n",
        "\n",
        "# Export the model to TFLite format\n",
        "model.export(format=\"tflite\")  # creates 'yolo11n_float32.tflite'\n",
        "\n",
        "# Load the exported TFLite model\n",
        "tflite_model = YOLO(\"/content/trained_model_saved_model/trained_model_float32.tflite/\")\n",
        "\n",
        "# Run inference\n",
        "results = tflite_model(\"https://ultralytics.com/images/bus.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITekby9hEAqS"
      },
      "source": [
        "#Fix the custom ops issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zFs7oZwEFRD"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from ultralytics import YOLO\n",
        "import tensorflow as tf\n",
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "# **Step 1: Export YOLOv8 Model to ONNX**\n",
        "model = YOLO('trained_model.pt')  # Replace with your YOLOv8 .pt model\n",
        "model.export(format='onnx', opset=12)  # Adjust opset if necessary\n",
        "\n",
        "# **Step 2: Convert ONNX to TensorFlow SavedModel**\n",
        "onnx_model = onnx.load(\"trained_model.onnx\")\n",
        "tf_rep = prepare(onnx_model)\n",
        "\n",
        "# Ensure all variables are in float32\n",
        "for var in tf_rep.tf_module.variables:\n",
        "    var.assign(tf.cast(var, tf.float32))\n",
        "\n",
        "# Save the TensorFlow model in float32 format\n",
        "tf_rep.export_graph(\"/content/saved_model_dir_float32\")\n",
        "\n",
        "# **Step 3: Replace Unsupported Ops in TensorFlow Model**\n",
        "def replace_cast_op(saved_model_dir, output_dir):\n",
        "    # Load the model\n",
        "    loaded_model = tf.saved_model.load(saved_model_dir)\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 640, 640, 3], dtype=tf.float32)])\n",
        "    def modified_model(input_tensor):\n",
        "        # Replace unsupported `Cast` operations\n",
        "        return loaded_model.signatures['serving_default'](input_tensor=tf.cast(input_tensor, tf.float32))\n",
        "\n",
        "    # Save the modified model\n",
        "    tf.saved_model.save(loaded_model, output_dir, signatures={'serving_default': modified_model.get_concrete_function()})\n",
        "\n",
        "# Replace unsupported ops\n",
        "replace_cast_op('/content/saved_model_dir_float32', '/content/saved_model_dir_fixed')\n",
        "\n",
        "# **Step 4: Convert TensorFlow Model to TensorFlow Lite**\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/saved_model_dir_fixed')\n",
        "\n",
        "# Enable optimization (optional)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Enable TensorFlow Select Ops to resolve `Cast` issue\n",
        "converter.allow_custom_ops = True\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('tastetrace.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# **Step 5: Verify the Converted TFLite Model**\n",
        "interpreter = tf.lite.Interpreter(model_path='tastetrace_fixed.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "print(\"TFLite model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QbzQJO6hGGqJ"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install tensorflow-addons==0.20.0\n",
        "!pip install tensorflow==2.12"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
